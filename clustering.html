
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Clustering {#clustering} &#8212; DSCΙ 100</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/style.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Statistical inference {#inference}" href="inference.html" />
    <link rel="prev" title="Regression II: linear regression {#regression2}" href="regression2.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">DSCΙ 100</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   R and the Tidyverse hello worldV
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  First draft
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="preface-text.html">
   Preface {-}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="setup.html">
   Setting up your computer {#move-to-your-own-machine}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="reading.html">
   Reading in data locally and from the web {#reading}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wrangling.html">
   Cleaning and wrangling data {#wrangling}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="viz.html">
   Effective data visualization {#viz}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="version-control.html">
   Collaboration with version control {#Getting-started-with-version-control}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="classification1.html">
   Classification I: training &amp; predicting {#classification}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="classification2.html">
   Classification II: evaluation &amp; tuning {#classification2}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regression1.html">
   Regression I: K-nearest neighbors {#regression1}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regression2.html">
   Regression II: linear regression {#regression2}
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Clustering {#clustering}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="inference.html">
   Statistical inference {#inference}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="references.html">
   References {-}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="acknowledgements.html">
   Acknowledgments {-}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="authors.html">
   About the authors {-}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="appendixA.html">
   (APPENDIX) Appendix {-}
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="_sources/clustering.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/clustering.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/phaustin/eosc211_students/e211_live_main?urlpath=tree/clustering.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        <a class="jupyterhub-button" href="https://eosc211.jupyterhub.eoas.ubc.ca/jupyter/hub/user-redirect/git-pull?repo=https://github.com/phaustin/eosc211_students&urlpath=tree/eosc211_students/clustering.ipynb&branch=e211_live_main"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch JupyterHub" data-toggle="tooltip"
                data-placement="left"><img class="jupyterhub-button-logo"
                    src="_static/images/logo_jupyterhub.svg"
                    alt="Interact on JupyterHub">JupyterHub</button></a>
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chapter-learning-objectives">
   Chapter learning objectives
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#clustering">
   Clustering
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k-means">
   K-means
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#measuring-cluster-quality">
     Measuring cluster quality
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-clustering-algorithm">
     The clustering algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-restarts">
     Random restarts
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#choosing-k">
     Choosing K
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-pre-processing-for-k-means">
   Data pre-processing for K-means
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k-means-in-python">
   K-means in Python
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#additional-resources">
   Additional resources
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Clustering {#clustering}</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chapter-learning-objectives">
   Chapter learning objectives
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#clustering">
   Clustering
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k-means">
   K-means
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#measuring-cluster-quality">
     Measuring cluster quality
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-clustering-algorithm">
     The clustering algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-restarts">
     Random restarts
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#choosing-k">
     Choosing K
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-pre-processing-for-k-means">
   Data pre-processing for K-means
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k-means-in-python">
   K-means in Python
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#additional-resources">
   Additional resources
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="clustering-clustering">
<h1>Clustering {#clustering}<a class="headerlink" href="#clustering-clustering" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">make_column_transformer</span>

<span class="kn">import</span> <span class="nn">altair</span> <span class="k">as</span> <span class="nn">alt</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>As part of exploratory data analysis, it is often helpful to see if there are
meaningful subgroups (or <em>clusters</em>) in the data.
This grouping can be used for many purposes,
such as generating new questions or improving predictive analyses.
This chapter provides an introduction to clustering
using the K-means algorithm,
including techniques to choose the number of clusters.</p>
</div>
<div class="section" id="chapter-learning-objectives">
<h2>Chapter learning objectives<a class="headerlink" href="#chapter-learning-objectives" title="Permalink to this headline">¶</a></h2>
<p>By the end of the chapter, readers will be able to do the following:</p>
<ul class="simple">
<li><p>Describe a case where clustering is appropriate,
and what insight it might extract from the data.</p></li>
<li><p>Explain the K-means clustering algorithm.</p></li>
<li><p>Interpret the output of a K-means analysis.</p></li>
<li><p>Differentiate between clustering and classification.</p></li>
<li><p>Identify when it is necessary to scale variables before clustering and do this using Python</p></li>
<li><p>Perform k-means clustering in Python using <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code></p></li>
<li><p>Use the elbow method to choose the number of clusters for K-means.</p></li>
<li><p>Visualize the output of k-means clustering in Python using a coloured scatter plot</p></li>
<li><p>Describe advantages, limitations and assumptions of the kmeans clustering algorithm.</p></li>
</ul>
</div>
<div class="section" id="clustering">
<h2>Clustering<a class="headerlink" href="#clustering" title="Permalink to this headline">¶</a></h2>
<p>Clustering \index{clustering} is a data analysis task
involving separating a data set into subgroups of related data.
For example, we might use clustering to separate a
data set of documents into groups that correspond to topics, a data set of
human genetic information into groups that correspond to ancestral
subpopulations, or a data set of online customers into groups that correspond
to purchasing behaviors.  Once the data are separated, we can, for example,
use the subgroups to generate new questions about the data and follow up with a
predictive modeling exercise. In this course, clustering will be used only for
exploratory analysis, i.e., uncovering patterns in the data.</p>
<p>Note that clustering is a fundamentally different kind of task
than classification or regression.
In particular, both classification and regression are <em>supervised tasks</em>
\index{classification}\index{regression}\index{supervised}
where there is a <em>response variable</em> (a category label or value),
and we have examples of past data with labels/values
that help us predict those of future data.
By contrast, clustering is an <em>unsupervised task</em>,
\index{unsupervised} as we are trying to understand
and examine the structure of data without any response variable labels
or values to help us.
This approach has both advantages and disadvantages.
Clustering requires no additional annotation or input on the data.
For example, it would be nearly impossible to annotate
all the articles on Wikipedia with human-made topic labels.
However, we can still cluster the articles without this information
to find groupings corresponding to topics automatically.</p>
<p>Given that there is no response variable, it is not as easy to evaluate
the “quality” of a clustering.  With classification, we can use a test data set
to assess prediction performance. In clustering, there is not a single good
choice for evaluation. In this book, we will use visualization to ascertain the
quality of a clustering, and leave rigorous evaluation for more advanced
courses.</p>
<p>As in the case of classification,
there are many possible methods that we could use to cluster our observations
to look for subgroups.
In this book, we will focus on the widely used K-means \index{K-means} algorithm [&#64;kmeans].
In your future studies, you might encounter hierarchical clustering,
principal component analysis, multidimensional scaling, and more;
see the additional resources section at the end of this chapter
for where to begin learning more about these other methods.</p>
<p>\newpage</p>
<blockquote>
<div><p><strong>Note:</strong> There are also so-called <em>semisupervised</em> tasks, \index{semisupervised}
where only some of the data come with response variable labels/values,
but the vast majority don’t.
The goal is to try to uncover underlying structure in the data
that allows one to guess the missing labels.
This sort of task is beneficial, for example,
when one has an unlabeled data set that is too large to manually label,
but one is willing to provide a few informative example labels as a “seed”
to guess the labels for all the data.</p>
</div></blockquote>
<p><strong>An illustrative example</strong></p>
<p>Here we will present an illustrative example using a data set \index{Palmer penguins} from
<a class="reference external" href="https://allisonhorst.github.io/palmerpenguins/">the <code class="docutils literal notranslate"><span class="pre">palmerpenguins</span></code> R package</a> [&#64;palmerpenguins]. This
data set was collected by Dr. Kristen Gorman and
the Palmer Station, Antarctica Long Term Ecological Research Site, and includes
measurements for adult penguins found near there [&#64;penguinpaper]. We have
modified the data set for use in this chapter. Here we will focus on using two
variables—penguin bill and flipper length, both in millimeters—to determine whether
there are distinct types of penguins in our data.
Understanding this might help us with species discovery and classification in a data-driven
way.</p>
<div class="figure align-default" id="penguins">
<a class="reference internal image-reference" href="_images/gentoo.jpg"><img alt="_images/gentoo.jpg" src="_images/gentoo.jpg" style="height: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 1 </span><span class="caption-text">Gentoo penguin.</span><a class="headerlink" href="#penguins" title="Permalink to this image">¶</a></p>
</div>
<p>To learn about K-means clustering
we will work with <code class="docutils literal notranslate"><span class="pre">penguin_data</span></code> in this chapter.
<code class="docutils literal notranslate"><span class="pre">penguin_data</span></code> is a subset of 18 observations of the original data,
which has already been standardized
(remember from Chapter &#64;ref(classification)
that scaling is part of the standardization process).
We will discuss scaling for K-means in more detail later in this chapter.
\index{mutate}\index{read function!read_csv}</p>
<p>Before we get started, we will set a random seed.
This will ensure that our analysis will be reproducible.
As we will learn in more detail later in the chapter,
setting the seed here is important
because the K-means clustering algorithm uses random numbers.</p>
<p>\index{seed!set.seed}</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we can load and preview the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">penguin_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/penguins_standardized.csv&quot;</span><span class="p">)</span>
<span class="n">penguin_data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>flipper_length_standardized</th>
      <th>bill_length_standardized</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.189773</td>
      <td>-0.641361</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-1.328412</td>
      <td>-1.144917</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.921755</td>
      <td>-1.517922</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.921755</td>
      <td>-1.107617</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-1.409743</td>
      <td>-0.846513</td>
    </tr>
    <tr>
      <th>5</th>
      <td>-0.677761</td>
      <td>-0.641361</td>
    </tr>
    <tr>
      <th>6</th>
      <td>-0.271104</td>
      <td>-1.238168</td>
    </tr>
    <tr>
      <th>7</th>
      <td>-0.433767</td>
      <td>-0.902464</td>
    </tr>
    <tr>
      <th>8</th>
      <td>1.192860</td>
      <td>0.720106</td>
    </tr>
    <tr>
      <th>9</th>
      <td>1.355522</td>
      <td>0.645505</td>
    </tr>
    <tr>
      <th>10</th>
      <td>1.355522</td>
      <td>0.962559</td>
    </tr>
    <tr>
      <th>11</th>
      <td>1.762179</td>
      <td>0.440353</td>
    </tr>
    <tr>
      <th>12</th>
      <td>1.111528</td>
      <td>1.205012</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.786203</td>
      <td>0.123299</td>
    </tr>
    <tr>
      <th>14</th>
      <td>-0.271104</td>
      <td>0.626855</td>
    </tr>
    <tr>
      <th>15</th>
      <td>-0.271104</td>
      <td>0.757407</td>
    </tr>
    <tr>
      <th>16</th>
      <td>-0.108442</td>
      <td>1.783170</td>
    </tr>
    <tr>
      <th>17</th>
      <td>-0.759092</td>
      <td>0.776057</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Next, we can create a scatter plot using this data set
to see if we can detect subtypes or groups in our data set.</p>
<p>\newpage</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scatter_plot</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">penguin_data</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Scatter plot of standardized bill length versus standardized flipper length.&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">mark_circle</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s2">&quot;flipper_length_standardized&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Flipper Length (standardized)&quot;</span><span class="p">),</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s2">&quot;bill_length_standardized&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Bill Length (standardized)&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">configure_axis</span><span class="p">(</span>
    <span class="n">labelFontSize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
    <span class="n">titleFontSize</span><span class="o">=</span><span class="mi">12</span>
<span class="p">)</span><span class="o">.</span><span class="n">configure_title</span><span class="p">(</span><span class="n">fontSize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
 
</pre></div>
</div>
</div>
</div>
<div class="figure align-default" id="scatter-plot" style="width: 700px">
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text">Scatter plot of standardized bill length versus standardized flipper length.</span><a class="headerlink" href="#scatter-plot" title="Permalink to this image">¶</a></p>
</div>
<p>Based \index{ggplot}\index{ggplot!geom_point} on the visualization
in Figure &#64;ref(fig:10-toy-example-plot),
we might suspect there are a few subtypes of penguins within our data set.
We can see roughly 3 groups of observations in Figure &#64;ref(fig:10-toy-example-plot),
including:</p>
<ol class="simple">
<li><p>a small flipper and bill length group,</p></li>
<li><p>a small flipper length, but large bill length group, and</p></li>
<li><p>a large  flipper and bill length group.</p></li>
</ol>
<p>Data visualization is a great tool to give us a rough sense of such patterns
when we have a small number of variables.
But if we are to group data—and select the number of groups—as part of
a reproducible analysis, we need something a bit more automated.
Additionally, finding groups via visualization becomes more difficult
as we increase the number of variables we consider when clustering.
The way to rigorously separate the data into groups
is to use a clustering algorithm.
In this chapter, we will focus on the <em>K-means</em> algorithm,
\index{K-means} a widely used and often very effective clustering method,
combined with the <em>elbow method</em> \index{elbow method}
for selecting the number of clusters.
This procedure will separate the data into groups;
Figure &#64;ref(fig:10-toy-example-clustering) shows these groups
denoted by colored scatter points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span> <span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="s2">&quot;brown&quot;</span><span class="p">]</span>

<span class="n">colored_scatter_plot</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Scatter plot of standardized bill length versus standardized flipper length with colored groups.&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">mark_circle</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s2">&quot;flipper_length_standardized&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Flipper Length (standardized)&quot;</span><span class="p">),</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s2">&quot;bill_length_standardized&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Bill Length (standardized)&quot;</span><span class="p">),</span>
    <span class="n">color</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Color</span><span class="p">(</span><span class="s1">&#39;cluster:N&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Scale</span><span class="p">(</span><span class="nb">range</span><span class="o">=</span><span class="n">colors</span><span class="p">)))</span><span class="o">.</span><span class="n">configure_axis</span><span class="p">(</span>
    <span class="n">labelFontSize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
    <span class="n">titleFontSize</span><span class="o">=</span><span class="mi">12</span>
<span class="p">)</span><span class="o">.</span><span class="n">configure_title</span><span class="p">(</span><span class="n">fontSize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
 
 
</pre></div>
</div>
</div>
</div>
<div class="figure align-default" id="colored-scatter-plot" style="width: 700px">
<p class="caption"><span class="caption-number">Fig. 3 </span><span class="caption-text">Scatter plot of standardized bill length versus standardized flipper length with colored groups.</span><a class="headerlink" href="#colored-scatter-plot" title="Permalink to this image">¶</a></p>
</div>
<p>What are the labels for these groups? Unfortunately, we don’t have any. K-means,
like almost all clustering algorithms, just outputs meaningless “cluster labels”
that are typically whole numbers: 0, 1, 2, 3, etc. But in a simple case like this,
where we can easily visualize the clusters on a scatter plot, we can give
human-made labels to the groups using their positions on
the plot:</p>
<ul class="simple">
<li><p>small flipper length and small bill length (<font color="#D55E00">orange cluster</font>),</p></li>
<li><p>small flipper length and large bill length (<font color="#0072B2">blue cluster</font>).</p></li>
<li><p>and large flipper length and large bill  length (<font color="#F0E442">yellow cluster</font>).</p></li>
</ul>
<p>Once we have made these determinations, we can use them to inform our species
classifications or ask further questions about our data. For example, we might
be interested in understanding the relationship between flipper length and bill
length, and that relationship may differ depending on the type of penguin we
have.</p>
</div>
<div class="section" id="k-means">
<h2>K-means<a class="headerlink" href="#k-means" title="Permalink to this headline">¶</a></h2>
<div class="section" id="measuring-cluster-quality">
<h3>Measuring cluster quality<a class="headerlink" href="#measuring-cluster-quality" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">clus</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;cluster&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s2">&quot;bill_length_standardized&quot;</span><span class="p">,</span> <span class="s2">&quot;flipper_length_standardized&quot;</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<p>The K-means algorithm is a procedure that groups data into K clusters.
It starts with an initial clustering of the data, and then iteratively
improves it by making adjustments to the assignment of data
to clusters until it cannot improve any further. But how do we measure
the “quality” of a clustering, and what does it mean to improve it?
In K-means clustering, we measure the quality of a cluster by its
\index{within-cluster sum-of-squared-distances|see{WSSD}}\index{WSSD}
<em>within-cluster sum-of-squared-distances</em> (WSSD), also called <em>intertia</em>. Computing this involves two steps.
First, we find the cluster centers by computing the mean of each variable
over data points in the cluster. For example, suppose we have a
cluster containing four observations, and we are using two variables, <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, to cluster the data.
Then we would compute the coordinates, <span class="math notranslate nohighlight">\(\mu_x\)</span> and <span class="math notranslate nohighlight">\(\mu_y\)</span>, of the cluster center via</p>
<p><span class="math notranslate nohighlight">\(\mu_x = \frac{1}{4}(x_1+x_2+x_3+x_4) \quad \mu_y = \frac{1}{4}(y_1+y_2+y_3+y_4)\)</span></p>
<p>In the first cluster from the example, there are <span class="pasted-inline"><code class="output text_plain docutils literal notranslate"><span class="pre">4</span></code></span> data points. These are shown with their cluster center
(flipper_length_standardized = <span class="pasted-inline"><code class="output text_plain docutils literal notranslate"><span class="pre">-0.35</span></code></span> and bill_length_standardized = <span class="pasted-inline"><code class="output text_plain docutils literal notranslate"><span class="pre">0.99</span></code></span>) highlighted
in Figure &#64;ref(fig:10-toy-example-clus1-center).</p>
<p>(ref:10-toy-example-clus1-center) Cluster 1 from the <code class="docutils literal notranslate"><span class="pre">penguin_data</span></code> data set example. Observations are in blue, with the cluster center highlighted in red.</p>
<div class="figure align-default" id="toy-example-clus1-center-1">
<a class="reference internal image-reference" href="_images/toy-example-clus1-center-1.png"><img alt="_images/toy-example-clus1-center-1.png" src="_images/toy-example-clus1-center-1.png" style="height: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 4 </span><span class="caption-text">Cluster 1 from the penguin_data data set example. Observations are in blue, with the cluster center highlighted in red.</span><a class="headerlink" href="#toy-example-clus1-center-1" title="Permalink to this image">¶</a></p>
</div>
<p>The second step in computing the WSSD is to add up the squared distance
\index{distance!K-means} between each point in the cluster
and the cluster center.
We use the straight-line / Euclidean distance formula
that we learned about in Chapter &#64;ref(classification).
In the <span class="pasted-inline"><code class="output text_plain docutils literal notranslate"><span class="pre">4</span></code></span>-observation cluster example above,
we would compute the WSSD <span class="math notranslate nohighlight">\(S^2\)</span> via</p>
<p><span class="math notranslate nohighlight">\(S^2 = \left((x_1 - \mu_x)^2 + (y_1 - \mu_y)^2\right) + \left((x_2 - \mu_x)^2 + (y_2 - \mu_y)^2\right) + \left((x_3 - \mu_x)^2 + (y_3 - \mu_y)^2\right)  +  \left((x_4 - \mu_x)^2 + (y_4 - \mu_y)^2\right)\)</span></p>
<p>These distances are denoted by lines in Figure &#64;ref(fig:10-toy-example-clus1-dists) for the first cluster of the penguin data example.</p>
<p>(ref:10-toy-example-clus1-dists) Cluster 1 from the <code class="docutils literal notranslate"><span class="pre">penguin_data</span></code> data set example. Observations are in blue, with the cluster center highlighted in red. The distances from the observations to the cluster center are represented as black lines.</p>
<div class="figure align-default" id="toy-example-clus1-dists-1">
<a class="reference internal image-reference" href="_images/toy-example-clus1-dists-1.png"><img alt="_images/toy-example-clus1-dists-1.png" src="_images/toy-example-clus1-dists-1.png" style="height: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 5 </span><span class="caption-text">Cluster 1 from the penguin_data data set example. Observations are in blue, with the cluster center highlighted in red. The distances from the observations to the cluster center are represented as black lines.</span><a class="headerlink" href="#toy-example-clus1-dists-1" title="Permalink to this image">¶</a></p>
</div>
<p>The larger the value of <span class="math notranslate nohighlight">\(S^2\)</span>, the more spread out the cluster is, since large <span class="math notranslate nohighlight">\(S^2\)</span> means that points are far from the cluster center.
Note, however, that “large” is relative to <em>both</em> the scale of the variables for clustering <em>and</em> the number of points in the cluster. A cluster where points are very close to the center might still have a large <span class="math notranslate nohighlight">\(S^2\)</span> if there are many data points in the cluster.</p>
<p>After we have calculated the WSSD for all the clusters,
we sum them together to get the <em>total WSSD</em>.
For our example,
this means adding up all the squared distances for the 18 observations.
These distances are denoted by black lines in
Figure &#64;ref(fig:10-toy-example-all-clus-dists).</p>
<p>(ref:10-toy-example-all-clus-dists) All clusters from the <code class="docutils literal notranslate"><span class="pre">penguin_data</span></code> data set example. Observations are in orange, blue, and yellow with the cluster center highlighted in red. The distances from the observations to each of the respective cluster centers are represented as black lines.</p>
<div class="figure align-default" id="toy-example-all-clus-dists-1">
<a class="reference internal image-reference" href="_images/toy-example-all-clus-dists-1.png"><img alt="_images/toy-example-all-clus-dists-1.png" src="_images/toy-example-all-clus-dists-1.png" style="height: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 6 </span><span class="caption-text">All clusters from the penguin_data data set example. Observations are in orange, blue, and yellow with the cluster center highlighted in red. The distances from the observations to each of the respective cluster centers are represented as black lines.</span><a class="headerlink" href="#toy-example-all-clus-dists-1" title="Permalink to this image">¶</a></p>
</div>
<p>\newpage</p>
</div>
<div class="section" id="the-clustering-algorithm">
<h3>The clustering algorithm<a class="headerlink" href="#the-clustering-algorithm" title="Permalink to this headline">¶</a></h3>
<p>We begin the K-means \index{K-means!algorithm} algorithm by picking K,
and randomly assigning a roughly equal number of observations
to each of the K clusters.
An example random initialization is shown in Figure &#64;ref(fig:10-toy-kmeans-init).</p>
<div class="figure align-default" id="toy-kmeans-init-1">
<a class="reference internal image-reference" href="_images/toy-kmeans-init-1.png"><img alt="_images/toy-kmeans-init-1.png" src="_images/toy-kmeans-init-1.png" style="height: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 7 </span><span class="caption-text">Random initialization of labels.</span><a class="headerlink" href="#toy-kmeans-init-1" title="Permalink to this image">¶</a></p>
</div>
<p>Then K-means consists of two major steps that attempt to minimize the
sum of WSSDs over all the clusters, i.e., the \index{WSSD!total} <em>total WSSD</em>:</p>
<ol class="simple">
<li><p><strong>Center update:</strong> Compute the center of each cluster.</p></li>
<li><p><strong>Label update:</strong> Reassign each data point to the cluster with the nearest center.</p></li>
</ol>
<p>These two steps are repeated until the cluster assignments no longer change.
We show what the first four iterations of K-means would look like in<br />
Figure &#64;ref(fig:10-toy-kmeans-iter).
There each row corresponds to an iteration,
where the left column depicts the center update,
and the right column depicts the reassignment of data to clusters.</p>
<p>(ref:10-toy-kmeans-iter) First four iterations of K-means clustering on the <code class="docutils literal notranslate"><span class="pre">penguin_data</span></code> example data set. Each pair of plots corresponds to an iteration. Within the pair, the first plot depicts the center update, and the second plot depicts the reassignment of data to clusters. Cluster centers are indicated by larger points that are outlined in black.</p>
<div class="figure align-default" id="toy-kmeans-iter-1">
<a class="reference internal image-reference" href="_images/toy-kmeans-iter-1.png"><img alt="_images/toy-kmeans-iter-1.png" src="_images/toy-kmeans-iter-1.png" style="height: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 8 </span><span class="caption-text">First four iterations of K-means clustering on the penguin_data example data set. Each pair of plots corresponds to an iteration. Within the pair, the first plot depicts the center update, and the second plot depicts the reassignment of data to clusters. Cluster centers are indicated by larger points that are outlined in black.</span><a class="headerlink" href="#toy-kmeans-iter-1" title="Permalink to this image">¶</a></p>
</div>
<p>Note that at this point, we can terminate the algorithm since none of the assignments changed
in the fourth iteration; both the centers and labels will remain the same from this point onward.</p>
<blockquote>
<div><p><strong>Note:</strong> Is K-means <em>guaranteed</em> to stop at some point, or could it iterate forever? As it turns out,
thankfully, the answer is that K-means \index{K-means!termination} is guaranteed to stop after <em>some</em> number of iterations. For the interested reader, the
logic for this has three steps: (1) both the label update and the center update decrease total WSSD in each iteration,
(2) the total WSSD is always greater than or equal to 0, and (3) there are only a finite number of possible
ways to assign the data to clusters. So at some point, the total WSSD must stop decreasing, which means none of the assignments
are changing, and the algorithm terminates.</p>
</div></blockquote>
<p>What kind of data is suitable for K-means clustering?
In the simplest version of K-means clustering that we have presented here,
the straight-line distance is used to measure the
distance between observations and cluster centers.
This means that only quantitative data should be used with this algorithm.
There are variants on the K-means algorithm,
as well as other clustering algorithms entirely,
that use other distance metrics
to allow for non-quantitative data to be clustered.
These, however, are beyond the scope of this book.</p>
</div>
<div class="section" id="random-restarts">
<h3>Random restarts<a class="headerlink" href="#random-restarts" title="Permalink to this headline">¶</a></h3>
<p>Unlike the classification and regression models we studied in previous chapters, K-means \index{K-means!restart, nstart} can get “stuck” in a bad solution.
For example, Figure &#64;ref(fig:10-toy-kmeans-bad-init) illustrates an unlucky random initialization by K-means.</p>
<div class="figure align-default" id="toy-kmeans-bad-init-1">
<a class="reference internal image-reference" href="_images/toy-kmeans-bad-init-1.png"><img alt="_images/toy-kmeans-bad-init-1.png" src="_images/toy-kmeans-bad-init-1.png" style="height: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 9 </span><span class="caption-text">Random initialization of labels.</span><a class="headerlink" href="#toy-kmeans-bad-init-1" title="Permalink to this image">¶</a></p>
</div>
<p>Figure &#64;ref(fig:10-toy-kmeans-bad-iter) shows what the iterations of K-means would look like with the unlucky random initialization shown in Figure &#64;ref(fig:10-toy-kmeans-bad-init).</p>
<p>(ref:10-toy-kmeans-bad-iter) First five iterations of K-means clustering on the <code class="docutils literal notranslate"><span class="pre">penguin_data</span></code> example data set with a poor random initialization. Each pair of plots corresponds to an iteration. Within the pair, the first plot depicts the center update, and the second plot depicts the reassignment of data to clusters. Cluster centers are indicated by larger points that are outlined in black.</p>
<div class="figure align-default" id="toy-kmeans-bad-iter-1">
<a class="reference internal image-reference" href="_images/toy-kmeans-bad-iter-1.png"><img alt="_images/toy-kmeans-bad-iter-1.png" src="_images/toy-kmeans-bad-iter-1.png" style="height: 700px;" /></a>
<p class="caption"><span class="caption-number">Fig. 10 </span><span class="caption-text">First five iterations of K-means clustering on the penguin_data example data set with a poor random initialization. Each pair of plots corresponds to an iteration. Within the pair, the first plot depicts the center update, and the second plot depicts the reassignment of data to clusters. Cluster centers are indicated by larger points that are outlined in black.</span><a class="headerlink" href="#toy-kmeans-bad-iter-1" title="Permalink to this image">¶</a></p>
</div>
<p>This looks like a relatively bad clustering of the data, but K-means cannot improve it.
To solve this problem when clustering data using K-means, we should randomly re-initialize the labels a few times, run K-means for each initialization,
and pick the clustering that has the lowest final total WSSD.</p>
</div>
<div class="section" id="choosing-k">
<h3>Choosing K<a class="headerlink" href="#choosing-k" title="Permalink to this headline">¶</a></h3>
<p>In order to cluster data using K-means,
we also have to pick the number of clusters, K.
But unlike in classification, we have no response variable
and cannot perform cross-validation with some measure of model prediction error.
Further, if K is chosen too small, then multiple clusters get grouped together;
if K is too large, then clusters get subdivided.
In both cases, we will potentially miss interesting structure in the data.
Figure &#64;ref(fig:10-toy-kmeans-vary-k) illustrates the impact of K
on K-means clustering of our penguin flipper and bill length data
by showing the different clusterings for K’s ranging from 1 to 9.</p>
<div class="figure align-default" id="toy-kmeans-vary-k-1">
<a class="reference internal image-reference" href="_images/toy-kmeans-vary-k-1.png"><img alt="_images/toy-kmeans-vary-k-1.png" src="_images/toy-kmeans-vary-k-1.png" style="height: 700px;" /></a>
<p class="caption"><span class="caption-number">Fig. 11 </span><span class="caption-text">Clustering of the penguin data for K clusters ranging from 1 to 9. Cluster centers are indicated by larger points that are outlined in black.</span><a class="headerlink" href="#toy-kmeans-vary-k-1" title="Permalink to this image">¶</a></p>
</div>
<p>If we set K less than 3, then the clustering merges separate groups of data; this causes a large
total WSSD, since the cluster center (denoted by an “x”) is not close to any of the data in the cluster. On
the other hand, if we set K greater than 3, the clustering subdivides subgroups of data; this does indeed still
decrease the total WSSD, but by only a <em>diminishing amount</em>. If we plot the total WSSD versus the number of
clusters, we see that the decrease in total WSSD levels off (or forms an “elbow shape”) \index{elbow method} when we reach roughly
the right number of clusters (Figure &#64;ref(fig:10-toy-kmeans-elbow)).</p>
<div class="figure align-default" id="toy-kmeans-elbow-1">
<a class="reference internal image-reference" href="_images/toy-kmeans-elbow-1.png"><img alt="_images/toy-kmeans-elbow-1.png" src="_images/toy-kmeans-elbow-1.png" style="height: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 12 </span><span class="caption-text">Total WSSD for K clusters ranging from 1 to 9.</span><a class="headerlink" href="#toy-kmeans-elbow-1" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="data-pre-processing-for-k-means">
<h2>Data pre-processing for K-means<a class="headerlink" href="#data-pre-processing-for-k-means" title="Permalink to this headline">¶</a></h2>
<p>Similar to K-nearest neighbors classification and regression, K-means
clustering uses straight-line distance to decide which points are similar to
each other. Therefore, the <em>scale</em> of each of the variables in the data
will influence which cluster data points end up being assigned.
Variables with a large scale will have a much larger
effect on deciding cluster assignment than variables with a small scale.
To address this problem, we typically standardize \index{standardization!K-means}\index{K-means!standardization} our data before clustering,
which ensures that each variable has a mean of 0 and standard deviation of 1.
The <code class="docutils literal notranslate"><span class="pre">StandardScaler()</span></code> function in Python can be used to do this.
We show an example of how to use this function
below using an unscaled and unstandardized version of the data set in this chapter.</p>
<p>First, here is what the raw (i.e., not standardized) data looks like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">not_standardized_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/penguins_not_standardized.csv&quot;</span><span class="p">)</span>
<span class="n">not_standardized_data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>bill_length_mm</th>
      <th>flipper_length_mm</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>39.2</td>
      <td>196</td>
    </tr>
    <tr>
      <th>1</th>
      <td>36.5</td>
      <td>182</td>
    </tr>
    <tr>
      <th>2</th>
      <td>34.5</td>
      <td>187</td>
    </tr>
    <tr>
      <th>3</th>
      <td>36.7</td>
      <td>187</td>
    </tr>
    <tr>
      <th>4</th>
      <td>38.1</td>
      <td>181</td>
    </tr>
    <tr>
      <th>5</th>
      <td>39.2</td>
      <td>190</td>
    </tr>
    <tr>
      <th>6</th>
      <td>36.0</td>
      <td>195</td>
    </tr>
    <tr>
      <th>7</th>
      <td>37.8</td>
      <td>193</td>
    </tr>
    <tr>
      <th>8</th>
      <td>46.5</td>
      <td>213</td>
    </tr>
    <tr>
      <th>9</th>
      <td>46.1</td>
      <td>215</td>
    </tr>
    <tr>
      <th>10</th>
      <td>47.8</td>
      <td>215</td>
    </tr>
    <tr>
      <th>11</th>
      <td>45.0</td>
      <td>220</td>
    </tr>
    <tr>
      <th>12</th>
      <td>49.1</td>
      <td>212</td>
    </tr>
    <tr>
      <th>13</th>
      <td>43.3</td>
      <td>208</td>
    </tr>
    <tr>
      <th>14</th>
      <td>46.0</td>
      <td>195</td>
    </tr>
    <tr>
      <th>15</th>
      <td>46.7</td>
      <td>195</td>
    </tr>
    <tr>
      <th>16</th>
      <td>52.2</td>
      <td>197</td>
    </tr>
    <tr>
      <th>17</th>
      <td>46.8</td>
      <td>189</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>And then we apply the <code class="docutils literal notranslate"><span class="pre">StandardScaler()</span></code> function to both the columns in the data frame
using <code class="docutils literal notranslate"><span class="pre">fit_transform()</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">standardized_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">not_standardized_data</span><span class="p">),</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bill_length_mm&#39;</span><span class="p">,</span> <span class="s1">&#39;flipper_length_mm&#39;</span><span class="p">])</span>
    
<span class="n">standardized_data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>bill_length_mm</th>
      <th>flipper_length_mm</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.659955</td>
      <td>-0.195275</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-1.178110</td>
      <td>-1.366924</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-1.561929</td>
      <td>-0.948478</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-1.139728</td>
      <td>-0.948478</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.871055</td>
      <td>-1.450614</td>
    </tr>
    <tr>
      <th>5</th>
      <td>-0.659955</td>
      <td>-0.697410</td>
    </tr>
    <tr>
      <th>6</th>
      <td>-1.274065</td>
      <td>-0.278964</td>
    </tr>
    <tr>
      <th>7</th>
      <td>-0.928628</td>
      <td>-0.446343</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.740983</td>
      <td>1.227442</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.664219</td>
      <td>1.394821</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.990465</td>
      <td>1.394821</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.453119</td>
      <td>1.813267</td>
    </tr>
    <tr>
      <th>12</th>
      <td>1.239947</td>
      <td>1.143753</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.126873</td>
      <td>0.808996</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.645029</td>
      <td>-0.278964</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.779365</td>
      <td>-0.278964</td>
    </tr>
    <tr>
      <th>16</th>
      <td>1.834866</td>
      <td>-0.111586</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.798556</td>
      <td>-0.781100</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="k-means-in-python">
<h2>K-means in Python<a class="headerlink" href="#k-means-in-python" title="Permalink to this headline">¶</a></h2>
<p>To perform K-means clustering in Python, we use the <code class="docutils literal notranslate"><span class="pre">KMeans</span></code> function. \index{K-means!kmeans function} It takes at
least two arguments: the data frame containing the data you wish to cluster,
and K, the number of clusters (here we choose K = 3). Note that since the K-means
algorithm uses a random initialization of assignments, but since we set the random seed, the clustering will be reproducible.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="n">penguin_clust</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">standardized_data</span><span class="p">)</span>
<span class="n">penguin_clust</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KMeans(n_clusters=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">KMeans</label><div class="sk-toggleable__content"><pre>KMeans(n_clusters=3)</pre></div></div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">penguin_clust</span><span class="o">.</span><span class="n">inertia_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">penguin_clust</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">penguin_clust</span><span class="o">.</span><span class="n">n_iter_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">penguin_clust</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">penguin_clust</span><span class="o">.</span><span class="n">n_clusters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4.730719092276117
[[ 1.01445401 -0.36265343]
 [-1.03417802 -0.79156085]
 [ 0.70260136  1.29718342]]
2
[1 1 1 1 1 1 1 1 2 2 2 2 2 2 0 0 0 0]
3
</pre></div>
</div>
</div>
</div>
<p>As you can see above, the clustering object is returned by <code class="docutils literal notranslate"><span class="pre">KMeans</span></code>
has a lot of information that can be used to visualize the clusters, pick K, and evaluate the total WSSD.
To obtain the information in the clustering object, we will call the <code class="docutils literal notranslate"><span class="pre">predict</span></code> function. (We can also call the <code class="docutils literal notranslate"><span class="pre">labels_</span></code> attribute)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">penguin_clust</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">standardized_data</span><span class="p">)</span>
<span class="n">predictions</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], dtype=int32)
</pre></div>
</div>
</div>
</div>
<p>Let’s start by visualizing the clustering
as a colored scatter plot. To do that,
we will add a new column and store assign the above predictions to that. The final
data frame will contain the data and the cluster assignments for
each point:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">clustered_data</span> <span class="o">=</span> <span class="n">standardized_data</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">clusters</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">)</span>
<span class="n">clustered_data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>bill_length_mm</th>
      <th>flipper_length_mm</th>
      <th>clusters</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.659955</td>
      <td>-0.195275</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-1.178110</td>
      <td>-1.366924</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-1.561929</td>
      <td>-0.948478</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-1.139728</td>
      <td>-0.948478</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.871055</td>
      <td>-1.450614</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>-0.659955</td>
      <td>-0.697410</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>-1.274065</td>
      <td>-0.278964</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>-0.928628</td>
      <td>-0.446343</td>
      <td>1</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.740983</td>
      <td>1.227442</td>
      <td>2</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.664219</td>
      <td>1.394821</td>
      <td>2</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.990465</td>
      <td>1.394821</td>
      <td>2</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.453119</td>
      <td>1.813267</td>
      <td>2</td>
    </tr>
    <tr>
      <th>12</th>
      <td>1.239947</td>
      <td>1.143753</td>
      <td>2</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.126873</td>
      <td>0.808996</td>
      <td>2</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.645029</td>
      <td>-0.278964</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.779365</td>
      <td>-0.278964</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>1.834866</td>
      <td>-0.111586</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.798556</td>
      <td>-0.781100</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now that we have this information in a data frame, we can make a visualization
of the cluster assignments for each point, as shown in Figure &#64;ref(fig:10-plot-clusters-2).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cluster_plot</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">clustered_data</span><span class="p">)</span>
    <span class="o">.</span><span class="n">mark_circle</span><span class="p">()</span>
    <span class="o">.</span><span class="n">encode</span><span class="p">(</span>
         <span class="n">x</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s2">&quot;flipper_length_mm&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Flipper Length (standardized)&quot;</span><span class="p">),</span>
         <span class="n">y</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s2">&quot;bill_length_mm&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Bill Length (standardized)&quot;</span><span class="p">),</span>
    <span class="n">color</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Color</span><span class="p">(</span><span class="s2">&quot;clusters:O&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Cluster&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Scale</span><span class="p">(</span><span class="n">scheme</span><span class="o">=</span><span class="s2">&quot;dark2&quot;</span><span class="p">)),</span>
    <span class="p">)</span><span class="o">.</span><span class="n">properties</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
    <span class="o">.</span><span class="n">configure_axis</span><span class="p">(</span><span class="n">labelFontSize</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">titleFontSize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="figure align-default" id="cluster-plot" style="width: 700px">
<div class="cell_output docutils container">
<div class="output text_html">
<div id="altair-viz-e5005a57b07141bb8f309ee963a5ca46"></div>
<script type="text/javascript">
  var VEGA_DEBUG = (typeof VEGA_DEBUG == "undefined") ? {} : VEGA_DEBUG;
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== "altair-viz-e5005a57b07141bb8f309ee963a5ca46") {
      outputDiv = document.getElementById("altair-viz-e5005a57b07141bb8f309ee963a5ca46");
    }
    const paths = {
      "vega": "https://cdn.jsdelivr.net/npm//vega@5?noext",
      "vega-lib": "https://cdn.jsdelivr.net/npm//vega-lib?noext",
      "vega-lite": "https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext",
      "vega-embed": "https://cdn.jsdelivr.net/npm//vega-embed@6?noext",
    };

    function maybeLoadScript(lib, version) {
      var key = `${lib.replace("-", "")}_version`;
      return (VEGA_DEBUG[key] == version) ?
        Promise.resolve(paths[lib]) :
        new Promise(function(resolve, reject) {
          var s = document.createElement('script');
          document.getElementsByTagName("head")[0].appendChild(s);
          s.async = true;
          s.onload = () => {
            VEGA_DEBUG[key] = version;
            return resolve(paths[lib]);
          };
          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);
          s.src = paths[lib];
        });
    }

    function showError(err) {
      outputDiv.innerHTML = `<div class="error" style="color:red;">${err}</div>`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === "function" && define.amd) {
      requirejs.config({paths});
      require(["vega-embed"], displayChart, err => showError(`Error loading script: ${err.message}`));
    } else {
      maybeLoadScript("vega", "5")
        .then(() => maybeLoadScript("vega-lite", "4.17.0"))
        .then(() => maybeLoadScript("vega-embed", "6"))
        .catch(showError)
        .then(() => displayChart(vegaEmbed));
    }
  })({"config": {"view": {"continuousWidth": 400, "continuousHeight": 300}, "axis": {"labelFontSize": 20, "titleFontSize": 20}}, "data": {"name": "data-f9ba5372ad17f212b1b1827985326fef"}, "mark": "circle", "encoding": {"color": {"field": "clusters", "scale": {"scheme": "dark2"}, "title": "Cluster", "type": "ordinal"}, "x": {"field": "flipper_length_mm", "title": "Flipper Length (standardized)", "type": "quantitative"}, "y": {"field": "bill_length_mm", "title": "Bill Length (standardized)", "type": "quantitative"}}, "height": 400, "width": 400, "$schema": "https://vega.github.io/schema/vega-lite/v4.17.0.json", "datasets": {"data-f9ba5372ad17f212b1b1827985326fef": [{"bill_length_mm": -0.6599548427421356, "flipper_length_mm": -0.19527492342808553, "clusters": 1}, {"bill_length_mm": -1.1781100181422628, "flipper_length_mm": -1.366924463996594, "clusters": 1}, {"bill_length_mm": -1.561928666586801, "flipper_length_mm": -0.948478199507841, "clusters": 1}, {"bill_length_mm": -1.1397281532978085, "flipper_length_mm": -0.948478199507841, "clusters": 1}, {"bill_length_mm": -0.871055099386632, "flipper_length_mm": -1.4506137168943447, "clusters": 1}, {"bill_length_mm": -0.6599548427421356, "flipper_length_mm": -0.6974104408145891, "clusters": 1}, {"bill_length_mm": -1.2740646802533975, "flipper_length_mm": -0.27896417632583614, "clusters": 1}, {"bill_length_mm": -0.9286278966533135, "flipper_length_mm": -0.4463426821213374, "clusters": 1}, {"bill_length_mm": 0.7409832240804287, "flipper_length_mm": 1.2274423758336748, "clusters": 2}, {"bill_length_mm": 0.6642194943915214, "flipper_length_mm": 1.394820881629176, "clusters": 2}, {"bill_length_mm": 0.990465345569378, "flipper_length_mm": 1.394820881629176, "clusters": 2}, {"bill_length_mm": 0.453119237747025, "flipper_length_mm": 1.813267146117929, "clusters": 2}, {"bill_length_mm": 1.2399474670583288, "flipper_length_mm": 1.1437531229359241, "clusters": 2}, {"bill_length_mm": 0.12687338656916688, "flipper_length_mm": 0.8089961113449218, "clusters": 2}, {"bill_length_mm": 0.6450285619692941, "flipper_length_mm": -0.27896417632583614, "clusters": 0}, {"bill_length_mm": 0.7793650889248831, "flipper_length_mm": -0.27896417632583614, "clusters": 0}, {"bill_length_mm": 1.8348663721473635, "flipper_length_mm": -0.11158567053033494, "clusters": 0}, {"bill_length_mm": 0.7985560213471089, "flipper_length_mm": -0.7810996937123398, "clusters": 0}]}}, {"mode": "vega-lite"});
</script></div></div>
<p class="caption"><span class="caption-number">Fig. 13 </span><span class="caption-text">The data colored by the cluster assignments returned by K-means.</span><a class="headerlink" href="#cluster-plot" title="Permalink to this image">¶</a></p>
</div>
<p>As mentioned above, we also need to select K by finding
where the “elbow” occurs in the plot of total WSSD versus the number of clusters.
We can obtain the total WSSD (inertia) from our
clustering using <code class="docutils literal notranslate"><span class="pre">.inertia_</span></code> function. For example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">penguin_clust</span><span class="o">.</span><span class="n">inertia_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4.730719092276117
</pre></div>
</div>
</div>
</div>
<p>To calculate the total WSSD for a variety of Ks, we will
create a data frame with a column named <code class="docutils literal notranslate"><span class="pre">k</span></code> with rows containing
each value of K we want to run K-means with (here, 1 to 9).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">penguin_clust_ks</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;k&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span><span class="o">.</span><span class="n">transpose</span><span class="p">()})</span>
</pre></div>
</div>
</div>
</div>
<p>Then we use <code class="docutils literal notranslate"><span class="pre">assign()</span></code> to create a new column and <code class="docutils literal notranslate"><span class="pre">lambda</span></code> operator to apply the <code class="docutils literal notranslate"><span class="pre">KMeans</span></code> function
within each row to each K.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
<span class="n">penguin_clust_ks</span> <span class="o">=</span> <span class="n">penguin_clust_ks</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
    <span class="n">penguin_clusts</span><span class="o">=</span><span class="n">penguin_clust_ks</span><span class="p">[</span><span class="s1">&#39;k&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;random&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">standardized_data</span><span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>If we take a look at our data frame <code class="docutils literal notranslate"><span class="pre">penguin_clust_ks</span></code> now,
we see that it has two columns: one with the value for K,
and the other holding the clustering model objects.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">penguin_clust_ks</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>k</th>
      <th>penguin_clusts</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>KMeans(init='random', n_clusters=1, n_init=3)</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>KMeans(init='random', n_clusters=2, n_init=3)</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>KMeans(init='random', n_clusters=3, n_init=3)</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>KMeans(init='random', n_clusters=4, n_init=3)</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>KMeans(init='random', n_clusters=5, n_init=3)</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>KMeans(init='random', n_clusters=6, n_init=3)</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>KMeans(init='random', n_clusters=7, n_init=3)</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>KMeans(init='random', n_init=3)</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>KMeans(init='random', n_clusters=9, n_init=3)</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>If we wanted to get one of the clusterings out of the column in the data frame, we could use a familiar friend: <code class="docutils literal notranslate"><span class="pre">.iloc</span></code> property. And then to extract the <code class="docutils literal notranslate"><span class="pre">inertia</span></code> or any other attribute of the cluster object, we can simply access it using the <code class="docutils literal notranslate"><span class="pre">.</span></code> operator. Below, we will extract the details of the cluster object, where <code class="docutils literal notranslate"><span class="pre">k=2</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">penguin_clust_ks</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;penguin_clusts&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KMeans(init=&#x27;random&#x27;, n_clusters=2, n_init=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">KMeans</label><div class="sk-toggleable__content"><pre>KMeans(init=&#x27;random&#x27;, n_clusters=2, n_init=3)</pre></div></div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">penguin_clust_ks</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;penguin_clusts&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">inertia_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>11.576264153631982
</pre></div>
</div>
</div>
</div>
<p>Next, we use <code class="docutils literal notranslate"><span class="pre">assign</span></code> again to add 2 new columns <code class="docutils literal notranslate"><span class="pre">inertia</span></code> and <code class="docutils literal notranslate"><span class="pre">n_iter</span></code><br />
to each of the K-means clustering objects to get the clustering statistics</p>
<p>This results in a data frame with 4 columns, one for K, one for the
K-means clustering objects, and 2 for the clustering statistics:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">penguin_clust_ks</span> <span class="o">=</span> <span class="n">penguin_clust_ks</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
    <span class="n">inertia</span><span class="o">=</span><span class="n">penguin_clust_ks</span><span class="p">[</span><span class="s2">&quot;penguin_clusts&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">inertia_</span><span class="p">),</span>
    <span class="n">n_iter</span><span class="o">=</span><span class="n">penguin_clust_ks</span><span class="p">[</span><span class="s2">&quot;penguin_clusts&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">n_iter_</span><span class="p">)</span>

<span class="p">)</span>
    


<span class="n">penguin_clust_ks</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>k</th>
      <th>penguin_clusts</th>
      <th>inertia</th>
      <th>n_iter</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>KMeans(init='random', n_clusters=1, n_init=3)</td>
      <td>36.000000</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>KMeans(init='random', n_clusters=2, n_init=3)</td>
      <td>11.576264</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>KMeans(init='random', n_clusters=3, n_init=3)</td>
      <td>4.730719</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>KMeans(init='random', n_clusters=4, n_init=3)</td>
      <td>4.046972</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>KMeans(init='random', n_clusters=5, n_init=3)</td>
      <td>2.362131</td>
      <td>4</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>KMeans(init='random', n_clusters=6, n_init=3)</td>
      <td>1.849067</td>
      <td>4</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>KMeans(init='random', n_clusters=7, n_init=3)</td>
      <td>2.415241</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>KMeans(init='random', n_init=3)</td>
      <td>1.035974</td>
      <td>4</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>KMeans(init='random', n_clusters=9, n_init=3)</td>
      <td>0.971380</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now that we have <code class="docutils literal notranslate"><span class="pre">inertia</span></code> and <code class="docutils literal notranslate"><span class="pre">k</span></code> as columns in a data frame, we can make a line plot
(Figure &#64;ref(fig:10-plot-choose-k)) and search for the “elbow” to find which value of K to use. We will drop the column <code class="docutils literal notranslate"><span class="pre">penguin_clusts</span></code> to make the plotting in altair feasible</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">penguin_clust_ks</span> <span class="o">=</span> <span class="n">penguin_clust_ks</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="s1">&#39;penguin_clusts&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot</span><span class="o">=</span><span class="p">(</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">penguin_clust_ks</span><span class="p">)</span>
    <span class="o">.</span><span class="n">mark_line</span><span class="p">(</span><span class="n">point</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="o">.</span><span class="n">encode</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;K&quot;</span><span class="p">),</span>
        <span class="n">y</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s2">&quot;inertia&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Total within-cluster sum of squares&quot;</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="o">.</span><span class="n">properties</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
    <span class="o">.</span><span class="n">configure_axis</span><span class="p">(</span><span class="n">labelFontSize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">titleFontSize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="figure align-default" id="plot" style="width: 700px">
<div class="cell_output docutils container">
<div class="output text_html">
<div id="altair-viz-1a7a6933255d4affa6d47fa522bf8339"></div>
<script type="text/javascript">
  var VEGA_DEBUG = (typeof VEGA_DEBUG == "undefined") ? {} : VEGA_DEBUG;
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== "altair-viz-1a7a6933255d4affa6d47fa522bf8339") {
      outputDiv = document.getElementById("altair-viz-1a7a6933255d4affa6d47fa522bf8339");
    }
    const paths = {
      "vega": "https://cdn.jsdelivr.net/npm//vega@5?noext",
      "vega-lib": "https://cdn.jsdelivr.net/npm//vega-lib?noext",
      "vega-lite": "https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext",
      "vega-embed": "https://cdn.jsdelivr.net/npm//vega-embed@6?noext",
    };

    function maybeLoadScript(lib, version) {
      var key = `${lib.replace("-", "")}_version`;
      return (VEGA_DEBUG[key] == version) ?
        Promise.resolve(paths[lib]) :
        new Promise(function(resolve, reject) {
          var s = document.createElement('script');
          document.getElementsByTagName("head")[0].appendChild(s);
          s.async = true;
          s.onload = () => {
            VEGA_DEBUG[key] = version;
            return resolve(paths[lib]);
          };
          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);
          s.src = paths[lib];
        });
    }

    function showError(err) {
      outputDiv.innerHTML = `<div class="error" style="color:red;">${err}</div>`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === "function" && define.amd) {
      requirejs.config({paths});
      require(["vega-embed"], displayChart, err => showError(`Error loading script: ${err.message}`));
    } else {
      maybeLoadScript("vega", "5")
        .then(() => maybeLoadScript("vega-lite", "4.17.0"))
        .then(() => maybeLoadScript("vega-embed", "6"))
        .catch(showError)
        .then(() => displayChart(vegaEmbed));
    }
  })({"config": {"view": {"continuousWidth": 400, "continuousHeight": 300}, "axis": {"labelFontSize": 15, "titleFontSize": 20}}, "data": {"name": "data-75c103bebf85d08a1a8f7d306c57c62d"}, "mark": {"type": "line", "point": true}, "encoding": {"x": {"field": "k", "title": "K", "type": "quantitative"}, "y": {"field": "inertia", "title": "Total within-cluster sum of squares", "type": "quantitative"}}, "height": 400, "width": 400, "$schema": "https://vega.github.io/schema/vega-lite/v4.17.0.json", "datasets": {"data-75c103bebf85d08a1a8f7d306c57c62d": [{"k": 1, "inertia": 35.99999999999999, "n_iter": 2}, {"k": 2, "inertia": 11.576264153631982, "n_iter": 3}, {"k": 3, "inertia": 4.730719092276117, "n_iter": 3}, {"k": 4, "inertia": 4.046971636164045, "n_iter": 2}, {"k": 5, "inertia": 2.3621308411553095, "n_iter": 4}, {"k": 6, "inertia": 1.8490674361826995, "n_iter": 4}, {"k": 7, "inertia": 2.4152411605291895, "n_iter": 3}, {"k": 8, "inertia": 1.0359735206766723, "n_iter": 4}, {"k": 9, "inertia": 0.9713796058938011, "n_iter": 2}]}}, {"mode": "vega-lite"});
</script></div></div>
<p class="caption"><span class="caption-number">Fig. 14 </span><span class="caption-text">A plot showing the total WSSD versus the number of clusters.</span><a class="headerlink" href="#plot" title="Permalink to this image">¶</a></p>
</div>
<p>It looks like 3 clusters is the right choice for this data.
But why is there a “bump” in the total WSSD plot here?
Shouldn’t total WSSD always decrease as we add more clusters?
Technically yes, but remember:  K-means can get “stuck” in a bad solution.
Unfortunately, for K = 7 we had an unlucky initialization
and found a bad clustering! \index{K-means!restart, nstart}
We can help prevent finding a bad clustering
by trying a few different random initializations
via the <code class="docutils literal notranslate"><span class="pre">nstart</span></code> argument (Figure &#64;ref(fig:10-choose-k-nstart)
shows a setup where we use 10 restarts).
When we do this, K-means clustering will be performed
the number of times specified by the <code class="docutils literal notranslate"><span class="pre">nstart</span></code> argument,
and R will return to us the best clustering from this.
The more times we perform K-means clustering,
the more likely we are to find a good clustering (if one exists).
What value should you choose for <code class="docutils literal notranslate"><span class="pre">nstart</span></code>? The answer is that it depends
on many factors: the size and characteristics of your data set,
as well as the speed and size of your computer.
The larger the <code class="docutils literal notranslate"><span class="pre">nstart</span></code> value the better from an analysis perspective,
but there is a trade-off that doing many clusterings
could take a long time.
So this is something that needs to be balanced.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">penguin_clust_ks</span> <span class="o">=</span> <span class="n">penguin_clust_ks</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
    <span class="n">penguin_clusts</span><span class="o">=</span><span class="n">penguin_clust_ks</span><span class="p">[</span><span class="s1">&#39;k&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;k-means++&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">standardized_data</span><span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="n">penguin_clust_ks</span> <span class="o">=</span> <span class="n">penguin_clust_ks</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
    <span class="n">inertia</span><span class="o">=</span><span class="n">penguin_clust_ks</span><span class="p">[</span><span class="s2">&quot;penguin_clusts&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">inertia_</span><span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="s1">&#39;penguin_clusts&#39;</span><span class="p">)</span>



<span class="n">elbow_plot</span><span class="o">=</span><span class="p">(</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">penguin_clust_ks</span><span class="p">)</span>
    <span class="o">.</span><span class="n">mark_line</span><span class="p">(</span><span class="n">point</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="o">.</span><span class="n">encode</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;K&quot;</span><span class="p">),</span>
        <span class="n">y</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s2">&quot;inertia&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Total within-cluster sum of squares&quot;</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="o">.</span><span class="n">properties</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
    <span class="o">.</span><span class="n">configure_axis</span><span class="p">(</span><span class="n">labelFontSize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">titleFontSize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="figure align-default" id="elbow-plot" style="width: 700px">
<div class="cell_output docutils container">
<div class="output text_html">
<div id="altair-viz-2709d4dcc178496c83b723319771cad4"></div>
<script type="text/javascript">
  var VEGA_DEBUG = (typeof VEGA_DEBUG == "undefined") ? {} : VEGA_DEBUG;
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== "altair-viz-2709d4dcc178496c83b723319771cad4") {
      outputDiv = document.getElementById("altair-viz-2709d4dcc178496c83b723319771cad4");
    }
    const paths = {
      "vega": "https://cdn.jsdelivr.net/npm//vega@5?noext",
      "vega-lib": "https://cdn.jsdelivr.net/npm//vega-lib?noext",
      "vega-lite": "https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext",
      "vega-embed": "https://cdn.jsdelivr.net/npm//vega-embed@6?noext",
    };

    function maybeLoadScript(lib, version) {
      var key = `${lib.replace("-", "")}_version`;
      return (VEGA_DEBUG[key] == version) ?
        Promise.resolve(paths[lib]) :
        new Promise(function(resolve, reject) {
          var s = document.createElement('script');
          document.getElementsByTagName("head")[0].appendChild(s);
          s.async = true;
          s.onload = () => {
            VEGA_DEBUG[key] = version;
            return resolve(paths[lib]);
          };
          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);
          s.src = paths[lib];
        });
    }

    function showError(err) {
      outputDiv.innerHTML = `<div class="error" style="color:red;">${err}</div>`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === "function" && define.amd) {
      requirejs.config({paths});
      require(["vega-embed"], displayChart, err => showError(`Error loading script: ${err.message}`));
    } else {
      maybeLoadScript("vega", "5")
        .then(() => maybeLoadScript("vega-lite", "4.17.0"))
        .then(() => maybeLoadScript("vega-embed", "6"))
        .catch(showError)
        .then(() => displayChart(vegaEmbed));
    }
  })({"config": {"view": {"continuousWidth": 400, "continuousHeight": 300}, "axis": {"labelFontSize": 15, "titleFontSize": 20}}, "data": {"name": "data-772b44915409097b31df568cff60d910"}, "mark": {"type": "line", "point": true}, "encoding": {"x": {"field": "k", "title": "K", "type": "quantitative"}, "y": {"field": "inertia", "title": "Total within-cluster sum of squares", "type": "quantitative"}}, "height": 400, "width": 400, "$schema": "https://vega.github.io/schema/vega-lite/v4.17.0.json", "datasets": {"data-772b44915409097b31df568cff60d910": [{"k": 1, "inertia": 35.99999999999999, "n_iter": 2}, {"k": 2, "inertia": 11.576264153631982, "n_iter": 3}, {"k": 3, "inertia": 4.730719092276117, "n_iter": 3}, {"k": 4, "inertia": 3.3436127899809343, "n_iter": 2}, {"k": 5, "inertia": 2.3621308411553095, "n_iter": 4}, {"k": 6, "inertia": 1.6783833850432375, "n_iter": 4}, {"k": 7, "inertia": 1.3087316666611315, "n_iter": 3}, {"k": 8, "inertia": 1.0659421816324675, "n_iter": 4}, {"k": 9, "inertia": 0.8002969878568551, "n_iter": 2}]}}, {"mode": "vega-lite"});
</script></div></div>
<p class="caption"><span class="caption-number">Fig. 15 </span><span class="caption-text">A plot showing the total WSSD versus the number of clusters when K-means is run with 10 restarts.</span><a class="headerlink" href="#elbow-plot" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<p>Practice exercises for the material covered in this chapter
can be found in the accompanying
<a class="reference external" href="https://github.com/UBC-DSCI/data-science-a-first-intro-worksheets#readme">worksheets repository</a>
in the “Clustering” row.
You can launch an interactive version of the worksheet in your browser by clicking the “launch binder” button.
You can also preview a non-interactive version of the worksheet by clicking “view worksheet.”
If you instead decide to download the worksheet and run it on your own machine,
make sure to follow the instructions for computer setup
found in Chapter &#64;ref(move-to-your-own-machine). This will ensure that the automated feedback
and guidance that the worksheets provide will function as intended.</p>
</div>
<div class="section" id="additional-resources">
<h2>Additional resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Chapter 10 of <em>An Introduction to Statistical
Learning</em> [&#64;james2013introduction] provides a
great next stop in the process of learning about clustering and unsupervised
learning in general. In the realm of clustering specifically, it provides a
great companion introduction to K-means, but also covers <em>hierarchical</em>
clustering for when you expect there to be subgroups, and then subgroups within
subgroups, etc., in your data. In the realm of more general unsupervised
learning, it covers <em>principal components analysis (PCA)</em>, which is a very
popular technique for reducing the number of predictors in a dataset.</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="regression2.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Regression II: linear regression {#regression2}</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="inference.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Statistical inference {#inference}</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By UBC<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>